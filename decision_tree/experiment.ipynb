{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b274572-9dc3-4da7-a925-4872df4bdd28",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "\n",
    "For this problem, you will be implementing a Decision Tree classifier that works on discrete (categorical) features. Although a relatively simple learning algorithm, the Decision Tree is often used as a fundamental building block for more powerful (and popular) models such as Random Forest and Gradient Boosted ensembles. \n",
    "\n",
    "You should base your solution on the [ID3](https://en.wikipedia.org/wiki/ID3_algorithmhttps://en.wikipedia.org/wiki/ID3_algorithm) algorithm. This is a basic tree-learning algorithm that greedly grows a tree based on _information gain_ (reduction in entropy). Please refer to Chapter 3 of _Machine Learning_ by Tom M. Mitchell for more details. \n",
    "\n",
    "\n",
    "We have provided some skeleton code for the classifier, along with a couple of utility functions in the [decision_tree.py](./decision_tree.py) module. Please fill out the functions marked with `TODO` and feel free to add extra constructor arguments as you see fit (just make sure the default constructor solves the first dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622ac96-3cf9-4e5a-b637-7788806be108",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34a3d5-dc59-46de-b607-34ef10349be2",
   "metadata": {},
   "source": [
    "We begin by loading necessary packages. Below follows a short description of the imported modules:\n",
    "\n",
    "- `numpy` is the defacto python package for numerical calculation. Most other numerical libraries (including pandas) is based on numpy.\n",
    "- `pandas` is a widely used package for manipulating (mostly) tabular data\n",
    "- `decision_tree` refers to the module in this folder that should be further implemented by you\n",
    "\n",
    "Note: The `%autoreload` statement is an [IPython magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html) that automatically reloads the newest version of all imported modules within the cell. This means that you can edit the `decision_tree.py` file and just rerun this cell to get the updated version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d91ba-2055-4e8a-ac4d-58644ca28cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import decision_tree as dt  # <-- Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2602f8-c86a-47cb-b947-56220f9e5f25",
   "metadata": {},
   "source": [
    "## [1] First Dataset\n",
    "\n",
    "The first dataset is a toy problem lifted from Table 3.2 in the Machine Learning textbook. The objective is to predict whether a given day is suitable for playing tennis based on several weather conditions. \n",
    "\n",
    "### [1.1] Load Data\n",
    "\n",
    "We begin by loading data from the .csv file located in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177a666-4c00-4586-88a7-44501e236964",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('data_1.csv')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e3743-b69a-477d-b7f2-ac06f1d86909",
   "metadata": {},
   "source": [
    "### [1.2] Fit and Evaluate Model\n",
    "\n",
    "Next we fit and evaluate a Decision Tree over the dataset. We first partition the data into the dependent (`y` = Play Tennis) and independent (`X` = everything else) variables. We then initialize a Decision Tree learner and fit it to all the data. Finally, we evaluate the model over the same data by calculating its accuracy, i.e. the fraction of correctly classified samples.\n",
    "\n",
    "Note that `.fit` and `.predict` will crash until you implement these two methods in [decision_tree.py](./decision_tree.py).\n",
    "\n",
    "Assuming that you've correctly implemented the ID3 algorithm as described in the course textbook, you should expect the model to perfectly fit the training data. That is, you should get a classification accuracy of 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a3a05-4de8-49d6-8596-48b02deb5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate independent (X) and dependent (y) variables\n",
    "X = data_1.drop(columns=['Play Tennis'])\n",
    "y = data_1['Play Tennis']\n",
    "\n",
    "# Create and fit a Decrision Tree classifier\n",
    "model_1 = dt.DecisionTree()  # <-- Should work with default constructor\n",
    "model_1.fit(X, y)\n",
    "\n",
    "# Verify that it perfectly fits the training set\n",
    "print(f'Accuracy: {dt.accuracy(y_true=y, y_pred=model_1.predict(X)) * 100 :.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f118d07-c1a0-4f85-8543-d7c9cb502bd1",
   "metadata": {},
   "source": [
    "### [1.3] Inspect Classification Rules\n",
    "\n",
    "A big advantage of Decision Trees is that they are relatively transparent learners. By this we mean that it is easy for an outside observer to analyse and understand how the model makes its decisions. The problem of being able to reason about how a machine learning model reasons is known as _Explainable AI_ and is often a desirable property of machine learning systems.\n",
    "\n",
    "Every time a Decision Tree is evaluated, the datapoint is compared against a set of nodes starting at the root of the tree and (typically) ending at one of the leaf nodes. An equivalent way to view this reasoning is as an implication rule ($A \\rightarrow B$) where the antecedent ($A$) is a conjunction of of attribute values and the consequent ($B$) is the predicted label. For instance, if a path down the tree first checks if Outlook=Rain, then checks if Wind=Strong, and then predicts Play Tennis=No, this line of reasoning can be represented as:\n",
    "\n",
    "- If $Outlook=Rain \\cap Wind=Strong \\rightarrow$ then predict $Play Tennis = No$\n",
    "\n",
    "We will leverage this property to export the decision tree you just created as a set of rules. For the subsequent cell to work, you must also have implemented the `.get_rules()` method in the provided boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620571c-c47e-4917-b649-8b5355b3abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rules, label in model_1.get_rules():\n",
    "    conjunction = ' ∩ '.join(f'{attr}={value}' for attr, value in rules)\n",
    "    print(f'{\"✅\" if label == \"Yes\" else \"❌\"} {conjunction} => {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b1ec9-f133-4569-b144-ec486cf941d3",
   "metadata": {},
   "source": [
    "## [2] Second Dataset\n",
    "\n",
    "The second dataset involves predicting whether an investment opportunity will result in a successful `Outcome` or not. To make this prediction, you are given a dataset of 200 historical$^1$ business ventures and their outcome, along with the following observed features:\n",
    "\n",
    "- Whether the business oportunity is in a lucurative market or not \n",
    "- Whether the presented business idea has a competitive advantage\n",
    "- Whether the second opinion from another investor is positive or not \n",
    "- The founder's previous experience with startups\n",
    "- The founder's [Zodiac Sign](https://en.wikipedia.org/wiki/Astrologyhttps://en.wikipedia.org/wiki/Astrology)\n",
    "\n",
    "---\n",
    "[1] Disclaimer: The dataset is not based on real-world business ventures. It is synthetic and generated by us. Also, it should not be considered financial advice.\n",
    "\n",
    "### [2.1] Load Data\n",
    "\n",
    "This dataset can also be found in a .csv file in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cad8c0-231d-4149-a378-24bfae38482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.read_csv('data_2.csv')\n",
    "data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc9f3a-1f01-4ec7-b706-2b5114cb7d78",
   "metadata": {},
   "source": [
    "### [2.2] Split Data\n",
    "\n",
    "We've also taken the liberty to pre-split the dataset into three different sets:\n",
    "\n",
    "- `train` contains 50 samples that you should use to generate the tree\n",
    "- `valid` contains 50 samples that you can use to evaluate different preprocessing methods and variations to the tree-learning algorithm.\n",
    "- `test` contains 100 samples and should only be used to evaluate the final model once you're done experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313d48e-0d2d-44f4-92dc-4b2eef22fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2_train = data_2.query('Split == \"train\"')\n",
    "data_2_valid = data_2.query('Split == \"valid\"')\n",
    "data_2_test = data_2.query('Split == \"test\"')\n",
    "X_train, y_train = data_2_train.drop(columns=['Outcome', 'Split']), data_2_train.Outcome\n",
    "X_valid, y_valid = data_2_valid.drop(columns=['Outcome', 'Split']), data_2_valid.Outcome\n",
    "X_test, y_test = data_2_test.drop(columns=['Outcome', 'Split']), data_2_test.Outcome\n",
    "data_2.Split.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a952ca-a144-4ea6-a806-e1d9df860f41",
   "metadata": {},
   "source": [
    "### [2.3] Fit and Evaluate Model\n",
    "\n",
    "You may notice that the basic ID3 algorithm you developed for the first dataset does not generalize well when applied straight to this problem. Feel free to add extra functionality to it and/or the data preprocessing pipeline that might improve performance on the validation (and ultimately test set). As a debugging reference; it is highly possible to obtain accuracies over the validation and test set ranging from mid ~80% to low ~90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a420f4c-71c5-4fc6-a60a-a87c66dcd5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model (TO TRAIN SET ONLY)\n",
    "model_2 = dt.DecisionTree()  # <-- Feel free to add hyperparameters \n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "print(f'Train: {dt.accuracy(y_train, model_2.predict(X_train)) * 100 :.1f}%')\n",
    "print(f'Valid: {dt.accuracy(y_valid, model_2.predict(X_valid)) * 100 :.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d50800-c89c-4695-9f2e-e7c33b597e6b",
   "metadata": {},
   "source": [
    "## [3] Further steps (optional)\n",
    "\n",
    "If you're done with the assignment but want to some more challenges; consider the following:\n",
    "\n",
    "- Make a Decision Tree learner that can handle numerical attributes\n",
    "- Make a Decision Tree learner that can handle numerical targets (regresion tree)\n",
    "- Try implementing [Random Forest](https://en.wikipedia.org/wiki/Random_forest) on top of your Decision Tree algorithm\n",
    "\n",
    "If you need more data for experimenting, UC Irvine hosts a [large repository](https://archive.ics.uci.edu/ml/datasets.php) of machine learning datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
